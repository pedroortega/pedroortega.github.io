<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://pedroortega.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pedroortega.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-03T20:22:57+00:00</updated><id>https://pedroortega.github.io/feed.xml</id><subtitle>Pedro A. Ortega&apos;s homepage. </subtitle><entry><title type="html">And, Or, and the two KL-projections</title><link href="https://pedroortega.github.io/blog/2023/andor/" rel="alternate" type="text/html" title="And, Or, and the two KL-projections"/><published>2023-12-03T15:12:00+00:00</published><updated>2023-12-03T15:12:00+00:00</updated><id>https://pedroortega.github.io/blog/2023/andor</id><content type="html" xml:base="https://pedroortega.github.io/blog/2023/andor/"><![CDATA[<blockquote> <p>I discuss the difference between minimizing the KL-divergence with respect to the first and second argument, and will conclude that they correspond to AND and OR operations on distributions, respectively.</p> </blockquote> <p>Oftentimes I see people wonder about the meaning of the two KL-projections:</p> \[\min_p D(p \| q) \qquad \text{versus} \qquad \min_p D(q \| p),\] <p>where of course (in the discrete case)</p> \[D(p \| q) := \sum_x p(x) \log \frac{ p(x) }{ q(x) }.\] <p>One popular intuitive explanation is that “the first finds the mode and the second the support”, motivated by depictions such as the following (from <a href="https://arxiv.org/abs/1804.00140">Manisha and Gujar, 2019</a>)</p> <div class="row mt-1"> <div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/forward-reverse-kl-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/forward-reverse-kl-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/forward-reverse-kl-1400.webp"/> <img src="/assets/img/forward-reverse-kl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Pedro A. Ortega at the NIH, Bethesda (Washington DC)" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Personally, I think this is a bit vague and unsatisfactory. I had battled with this distinction since my PhD studies, trying to understand it from multiple angles (notably, <a href="https://en.wikipedia.org/wiki/Information_geometry">information geometry</a>) but I was never really happy with the way the material was presented in the literature. I think the simple explanation below condenses a significant amount of insight without resorting to information geometry. I hope you agree!</p> <h2 id="a-tale-of-two-coordinate-systems">A tale of two coordinate systems</h2> <p>It’s pretty hard to understand the difference between the two projections just by applying them to two distributions \(p\) and \(q\). Rather, we’ll tackle this by looking at two ways of mixing multiple distributions.</p> <h3 id="linear-mixture">Linear mixture</h3> <p>Let’s say we have \(N\) distributions \(q_1, q_2, \ldots, q_N\) over a finite set \(X\). Given a set of positive weights \(w_1, w_2, \ldots, w_N\) that sum up to one, their <em>linear mixture</em> is</p> <p>\begin{equation} \label{eq:linear-mix} q(x) = \sum_i w_i q_i(x), \end{equation}</p> <p>The <em>linear mixture</em> expresses \(N\) mutually exclusive hypotheses \(q_i(x)\) that could be true with probabilities \(w_i\). That is, either \(q_1\) <strong>or</strong> \(q_2\) <strong>or</strong> … <strong>or</strong> \(q_N\) is true, with probability \(w_1\), \(w_2\), …, \(w_N\) respectively, expressing a <strong>disjunction</strong> of probability distribution.</p> <h3 id="exponential-mixture">Exponential mixture</h3> <p>Given a set of positive coefficients \(\alpha_1, \alpha_2, \ldots, \alpha_N\) (but not necessarily summing up to one), their <em>exponential mixture</em> (a.k.a. geometric mixture) is</p> <p>\begin{equation} \label{eq:exponential-mix} q(x) \propto \prod_i q_i(x)^{\alpha_i}. \end{equation}</p> <p>Note the exponential mixture needs to be normalized in order for the result \(q(x)\) to be a probability distribution.</p> <p>The <em>exponential mixture</em> expresses \(N\) constraints \(q_i(x)\) that must be true simultaneously with precisions \(\alpha_i\). That is, \(q_1\) <strong>and</strong> \(q_2\) <strong>and</strong> … <strong>and</strong> \(q_N\) are true, with precisions \(\alpha_1\), \(\alpha_2\), …, \(\alpha_N\) respectively, expressing a <strong>conjunction</strong> of probability distributions.</p> <h2 id="building-conjunctions-and-disjunctions">Building conjunctions and disjunctions</h2> <p>So now that we’ve seen how to express conjunctions and disjunctions of distributions as exponential and linear mixtures respectively, we’ll try to find objective functions (or divergence measures, if you prefer) to build them.</p> <p><strong>Disjunctions:</strong> If I have a set of alternative hypotheses \(q_1(x)\) through \(q_N(x)\) which are true with probabilities \(w_1\) through \(w_N\) respectively, how far off is \(p\) from this knowledge?</p> <p>The answer is</p> \[\sum_i w_i D(q_i \| p).\] <p>That is, using the KL-divergences where \(p\) is the second argument. Indeed, the minimizer is precisely the linear mixture:</p> <p>\begin{equation} \label{eq:build-linear} \arg\min_p \sum_i w_i D(q_i | p) = \sum_i w_i q_i(x). \end{equation}</p> <p><strong>Conjunctions:</strong> If I have a set of simultaneous constraints \(q_1(x)\) through \(q_N(x)\) with precisions \(\alpha_1\) through \(\alpha_N\), how far off is \(p\) from this knowlegde?</p> <p>The answer is</p> \[\sum_i \alpha_i D(p \| q_i).\] <p>That is, using the KL-divergences where \(p\) is in the first argument. And in fact, the minimizer precisely is the exponential mixture:</p> <p>\begin{equation} \label{eq:build-exponential} \arg\min_p \sum_i \alpha_i D(p | q_i) \propto \prod q_i(x)^{\alpha_i}. \end{equation}</p> <p>Equations \eqref{eq:build-linear} and \eqref{eq:build-exponential} form the core of my argument. Basically, we have found a relation between the two KL-projections and the two logical operators <strong>and</strong> and <strong>or</strong>. The two KL-divergences then measure the deviation from a conjunction and disjunction, respectively.</p> <h2 id="final-thoughts">Final thoughts</h2> <p>Let’s revisit the figure above:</p> <div class="row mt-1"> <div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/forward-reverse-kl-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/forward-reverse-kl-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/forward-reverse-kl-1400.webp"/> <img src="/assets/img/forward-reverse-kl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Pedro A. Ortega at the NIH, Bethesda (Washington DC)" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We are equipped with a new interpretation. For simplicity, let’s think of the blue distribution (\(p_d\)) as a mixture of two Gaussians with equal weights, but different locations. The next table gives an intuitive justification for the four cases.</p> <table> <thead> <tr> <th>OR</th> <th>AND</th> </tr> </thead> <tbody> <tr> <td>\(p_g\) does not capture the disjunction of the two distribution in the mixture very well, since the right peak isn’t covered by it. Hence, \(D(p_d | p_g)\) is high.</td> <td>\(p_g\) assigns little probability mass to any peak of the mixture, hence it is difficult for both \(p_g\) and \(p_d\) to be true at the same time: \(D(p_g | p_d)\) is high.</td> </tr> <tr> <td>\(p_g\) models the disjunction better and thus \(D(p_d | p_g)\) will be lower (albeit suboptimal).</td> <td>Since \(p_g\) covers one of the peaks of \(p_d\), it is a bit easier for both to be true simultaneously, meaning that \(D(p_g | p_d)\) is lower.</td> </tr> </tbody> </table> <p>As a final comment, I’d like to point out the connection to the Bayesian framework. Let’s consider two hypotheses \(h=1,2\) with i.i.d. likelihoods \(q(x|h=1)\) and \(q(x|h=2)\) and priors \(q(h=1)\) and \(q(h=2)\). Then, the Bayesian estimator is formed through a disjunction (i.e. one of them must be true):</p> \[p(x) = \arg\min_p \sum_i q(h=i) D\big(q(x \| h=i) | p(x) \big) = \sum_i q(h=i) q(x \| h=i).\] <p>Observations are constraints on the estimator. If we now observe $x=1$, then we incorporate this by forming a conjunction:</p> \[p(h=i) \propto \arg\min_p \sum_i D\big(p(h=i) \| q(x=1 | h=i)\big) \propto q(h=i) q(x=1 | h=i)\]]]></content><author><name>Pedro A. Ortega</name></author><category term="information-theory"/><category term="and-or,"/><category term="KL-projections"/><summary type="html"><![CDATA[Relating the two KL-projections to AND and OR]]></summary></entry></feed>