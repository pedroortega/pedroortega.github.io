<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://pedroortega.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://pedroortega.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-12-03T22:05:19+00:00</updated><id>https://pedroortega.github.io/feed.xml</id><subtitle>Pedro A. Ortega&apos;s homepage. </subtitle><entry><title type="html">And, Or, and the two KL-projections</title><link href="https://pedroortega.github.io/blog/2023/andor/" rel="alternate" type="text/html" title="And, Or, and the two KL-projections"/><published>2023-12-03T15:12:00+00:00</published><updated>2023-12-03T15:12:00+00:00</updated><id>https://pedroortega.github.io/blog/2023/andor</id><content type="html" xml:base="https://pedroortega.github.io/blog/2023/andor/"><![CDATA[<blockquote> <p>I discuss the difference between minimizing the KL-divergence with respect to the first and second argument, and will conclude that they correspond to AND and OR operations on distributions, respectively.</p> </blockquote> <p>Oftentimes I see people wonder about the meaning of the two KL-projections:</p> \[\min_p D(p \| q) \qquad \text{versus} \qquad \min_p D(q \| p),\] <p>where of course (in the discrete case)</p> \[D(p \| q) := \sum_x p(x) \log \frac{ p(x) }{ q(x) }.\] <p>One popular intuitive explanation is that “the first finds the mode and the second the support”, motivated by depictions such as the following (from <a href="https://arxiv.org/abs/1804.00140">Manisha and Gujar, 2019</a>)</p> <div class="row mt-1"> <div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/forward-reverse-kl-480.webp"/> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/forward-reverse-kl-800.webp"/> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/forward-reverse-kl-1400.webp"/> <img src="/assets/img/forward-reverse-kl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Pedro A. Ortega at the NIH, Bethesda (Washington DC)" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>Personally, I think this is a bit vague and unsatisfactory. I had battled with this distinction since my PhD studies, trying to understand it from multiple angles (notably, <a href="https://en.wikipedia.org/wiki/Information_geometry">information geometry</a>) but I was never really happy with the way the material was presented in the literature. I think the simple explanation below condenses a significant amount of insight without resorting to information geometry. I hope you agree!</p> <h2 id="a-tale-of-two-coordinate-systems">A tale of two coordinate systems</h2> <p>It’s pretty hard to understand the difference between the two projections just by applying them to two distributions \(p\) and \(q\). Rather, we’ll tackle this by looking at two ways of mixing multiple distributions.</p> <h3 id="linear-mixture">Linear mixture</h3> <p>Let’s say we have \(N\) distributions \(q_1, q_2, \ldots, q_N\) over a finite set \(X\). Given a set of positive weights \(w_1, w_2, \ldots, w_N\) that sum up to one, their <em>linear mixture</em> is</p> <p>\begin{equation} \label{eq:linear-mix} q(x) = \sum_i w_i q_i(x), \end{equation}</p> <p>The <em>linear mixture</em> expresses \(N\) mutually exclusive hypotheses \(q_i(x)\) that could be true with probabilities \(w_i\). That is, either \(q_1\) <strong>or</strong> \(q_2\) <strong>or</strong> … <strong>or</strong> \(q_N\) is true, with probability \(w_1\), \(w_2\), …, \(w_N\) respectively, expressing a <strong>disjunction</strong> of probability distribution.</p> <h3 id="exponential-mixture">Exponential mixture</h3> <p>Given a set of positive coefficients \(\alpha_1, \alpha_2, \ldots, \alpha_N\) (but not necessarily summing up to one), their <em>exponential mixture</em> (a.k.a. geometric mixture) is</p> <p>\begin{equation} \label{eq:exponential-mix} q(x) \propto \prod_i q_i(x)^{\alpha_i}. \end{equation}</p> <p>Note the exponential mixture needs to be normalized in order for the result \(q(x)\) to be a probability distribution.</p> <p>The <em>exponential mixture</em> expresses \(N\) constraints \(q_i(x)\) that must be true simultaneously with precisions \(\alpha_i\). That is, \(q_1\) <strong>and</strong> \(q_2\) <strong>and</strong> … <strong>and</strong> \(q_N\) are true, with precisions \(\alpha_1\), \(\alpha_2\), …, \(\alpha_N\) respectively, expressing a <strong>conjunction</strong> of probability distributions.</p> <h2 id="building-conjunctions-and-disjunctions">Building conjunctions and disjunctions</h2> <p>So now that we’ve seen how to express conjunctions and disjunctions of distributions as exponential and linear mixtures respectively, we’ll try to find objective functions (or divergence measures, if you prefer) to build them.</p> <p><strong>Disjunctions:</strong> If I have a set of alternative hypotheses \(q_1(x)\) through \(q_N(x)\) which are true with probabilities \(w_1\) through \(w_N\) respectively, how far off is \(p\) from this knowledge?</p> <p>The answer is</p> \[\sum_i w_i D(q_i \| p).\] <p>That is, using the KL-divergences where \(p\) is the second argument. Indeed, the minimizer is precisely the linear mixture:</p> <p>\begin{equation} \label{eq:build-linear} \arg\min_p \sum_i w_i D(q_i | p) = \sum_i w_i q_i(x). \end{equation}</p> <p><strong>Conjunctions:</strong> If I have a set of simultaneous constraints \(q_1(x)\) through \(q_N(x)\) with precisions \(\alpha_1\) through \(\alpha_N\), how far off is \(p\) from this knowlegde?</p> <p>The answer is</p> \[\sum_i \alpha_i D(p \| q_i).\] <p>That is, using the KL-divergences where \(p\) is in the first argument. And in fact, the minimizer precisely is the exponential mixture:</p> <p>\begin{equation} \label{eq:build-exponential} \arg\min_p \sum_i \alpha_i D(p | q_i) \propto \prod q_i(x)^{\alpha_i}. \end{equation}</p> <p>Equations \eqref{eq:build-linear} and \eqref{eq:build-exponential} form the core of my argument. Basically, we have found a relation between the two KL-projections and the two logical operators <strong>and</strong> and <strong>or</strong>. The two KL-divergences then measure the deviation from a conjunction and disjunction, respectively.</p> <h2 id="final-thoughts">Final thoughts</h2> <p>As a final comment, I’d like to point out the connection to the Bayesian framework. Let’s consider two hypotheses \(h \in \mathcal{H}\) with i.i.d. likelihoods \(q(x|h)\) and priors \(q(h)\).</p> <p>The first step is to build the estimator. Since we believe that one and only one hypothesis is true, then we take the disjunction over the likelihoods, where the weights are given by the prior probabilities:</p> \[\arg\min_p \sum_h q(h) D\big(q(x \| h) | p(x) \big) = \sum_h q(h) q(x|h).\] <p>The resulting estimator \(\sum_h q(h) q(x \mid h)\) is indeed the Bayesian estimator.</p> <p>The second step is to understand how to update our beliefs when making an observation. It turns out that an observation is a constraint on the prior beliefs, i.e. we obtain the posterior through a conjunction between the prior and the likelihood function. If \(\dot{x}\) is our observation, then the conjunction is</p> \[\arg\min_p \big\{ \alpha D\big(p(h) \| q(h)\big) + \beta D\big(p(h) \| q(\dot{x}|h)\big) \big\} \propto q(h)^\alpha q(\dot{x}|h)^\beta.\] <p>In this operation we use precisions equal to \(\alpha=\beta=1\). We can easily verify that the right hand side is indeed the Bayesian posterior after normalization.</p>]]></content><author><name>Pedro A. Ortega</name></author><category term="information-theory"/><category term="and-or,"/><category term="KL-projections"/><summary type="html"><![CDATA[Relating the two KL-projections to AND and OR]]></summary></entry></feed>