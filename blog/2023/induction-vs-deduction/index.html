<!DOCTYPE html> <html> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>What's the difference between universal computation and universal AI? | </title> <meta name="author" content="Pedro A. Ortega"> <meta name="description" content="Or, the inconvenient truth about induction"> <meta name="keywords" content="Pedro A. Ortega, bounded rationality, information theory, AGI"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.22.1/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?19f3075a2d19613090fe9e16b564e1fe" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/logo-large.png?85098070a1268506ed86aa466455c05c"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://pedroortega.github.io/blog/2023/induction-vs-deduction/"> <link rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?e74e74bf055e5729d44a7d031a5ca6a5" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="/assets/js/distillpub/template.v2.js"></script> <script src="/assets/js/distillpub/transforms.v2.js"></script> <script src="/assets/js/distillpub/overrides.js"></script> </head> <body> <d-front-matter> <script async type="text/json">{
      "title": "What's the difference between universal computation and universal AI?",
      "description": "Or, the inconvenient truth about induction",
      "published": "December 10, 2023",
      "authors": [
        {
          "author": "Pedro A. Ortega",
          "authorURL": "https://www.adaptiveagents.org",
          "affiliations": [
            {
              "name": "DAIOS",
              "url": ""
            }
          ]
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script> </d-front-matter> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"></a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about</a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/pictures/">pics</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="post distill"> <d-title> <h1>What's the difference between universal computation and universal AI?</h1> <p>Or, the inconvenient truth about induction</p> </d-title><d-byline></d-byline><d-article> <blockquote> <p>“Their words and thoughts were so clear, that whatever they said, came to be.”</p> <p>–<em>Popol Vuh (Mayan Creation Myth)</em></p> </blockquote> <blockquote> <p>“You insist that there is something that a machine can’t do. If you will tell me precisely what it is that a machine cannot do, then I can always make a machine which will do just that.”</p> <p>–<em>John von Neumann</em>.</p> </blockquote> <p>Like many, I believe that the hallmark of intelligence is the ability to recognize patterns. In fact, most of the other abilities commonly associated with intelligence, such as learning, planning, analogical reasoning, and more, can be seen as either directly connected to or dependent upon our capacity for pattern recognition.</p> <p>Let’s take for instance the sequence</p> \[\text{B123A, A231B, B312A, A123B, } \ldots\] <p>If you feel very strongly that the next string should be \(\text{B231A}\), then this is because you’ve recognized a <strong>pattern</strong>, even though this is most likely the first time you’ve seen this sequence. Once you’ve found the pattern, you then used it to predict the next string in the sequence.</p> <h2 id="pattern-recognition-and-computation">Pattern recognition and computation</h2> <p>Pattern recognition is related to computation. Let’s quickly run through them to set the stage.</p> <p><strong>Computation</strong> is the process that takes a program \(p\) as input and then uses it to mechanically generate an output sequence \(x_1 x_2 \ldots\). Thus, if you can express unambiguously how to do a task, then computation will perform the exact task you’ve described. The <a href="https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis" rel="external nofollow noopener" target="_blank">Church-Turing thesis</a> states that the mechanical processes are precisely those computable by a Turing machine.</p> <p><strong>Pattern recognition</strong> can be regarded as the inverse process: it finds a program \(p\) that would mechanically compute a given target sequence \(x_1 x_2 \ldots\). In other words,</p> \[\mathrm{Recognition}(x) = \mathrm{Computation}^{-1}(p).\] <p>In this sense, a pattern is a program.</p> <details><summary>How does this relate to logic?</summary> <p>Logic distinguishes between two types of reasoning processes:</p> <ul> <li> <strong>deduction</strong>, which generates a necessary conclusion given premises and rules;</li> <li> <strong>induction</strong>, which generates a non-necessary, general rule from observations.</li> </ul> <p>Here, “necessary” means that the outputs are implied by the inputs of the process. The outputs of the inductive process are thus only candidate general rules.</p> <p>Computation and pattern recognition are thus the computational analogs of deduction and induction respectively.</p> <p>There’s a third type of reasoning process: <strong>abduction</strong>. Abductive reasoning is the process of generating explanations for given observations. The above definition of induction encompasses it. Therefore, I won’t treat abduction as a separate case.</p> </details> <h2 id="computational-formalization">Computational formalization</h2> <p>Oftentimes I see people wondering about the meaning of the two KL-projections:</p> \[\min_p D(p \| q) \qquad \text{versus} \qquad \min_p D(q \| p),\] <p>where of course, in the discrete case,</p> \[D(p \| q) := \sum_x p(x) \log \frac{ p(x) }{ q(x) }.\] <p>One popular intuitive explanation is that “the first projection finds the mode and the second the support”, motivated by depictions such as the following (from <a href="https://arxiv.org/abs/1804.00140" rel="external nofollow noopener" target="_blank">Manisha and Gujar, 2019</a>)</p> <div class="row mt-1"> <div class="col-sm mt-1 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/forward-reverse-kl-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/forward-reverse-kl-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/forward-reverse-kl-1400.webp"></source> <img src="/assets/img/forward-reverse-kl.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" alt="Pedro A. Ortega at the NIH, Bethesda (Washington DC)" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> </div> </div> <p>Personally, I find this explanation somewhat unclear and unsatisfying. I’ve grappled with this distinction since my PhD studies, while attempting to comprehend it from various perspectives, including <a href="https://en.wikipedia.org/wiki/Information_geometry" rel="external nofollow noopener" target="_blank">information geometry</a>. However, I was never truly happy with how the material was presented in the literature. I think the simplified explanation below, which took me years to arrive to, encapsulates a substantial amount of insight without relying on the complexities of information geometry. I hope you agree!</p> <h2 id="a-tale-of-two-coordinate-systems">A tale of two coordinate systems</h2> <p>Understanding the distinction between the two projections solely by examining their application on two distributions can be quite challenging. Instead, a clearer grasp of the difference can be attained through the examination of mixture distributions. Let’s delve into this approach.</p> <h3 id="linear-mixture">Linear mixture</h3> <p>Let’s say we have \(N\) distributions \(q_1, q_2, \ldots, q_N\) over a finite set \(\mathcal{X}\). Given a set of positive weights \(w_1, w_2, \ldots, w_N\) that sum up to one, their <em>linear mixture</em> is</p> \[q(x) = \sum_i w_i q_i(x),\] <p>The <em>linear mixture</em> expresses \(N\) mutually exclusive hypotheses \(q_i(x)\) that could be true with probabilities \(w_i\). That is, either \(q_1\) <strong>or</strong> \(q_2\) <strong>or</strong> … <strong>or</strong> \(q_N\) is true, with probability \(w_1\), \(w_2\), …, \(w_N\) respectively, expressing a <strong>disjunction</strong> of probability distributions.</p> <h3 id="exponential-mixture">Exponential mixture</h3> <p>Given a set of positive coefficients \(\alpha_1, \alpha_2, \ldots, \alpha_N\) (not necessarily summing up to one), their <em>exponential mixture</em> (a.k.a. geometric mixture) is</p> \[q(x) \propto \prod_i q_i(x)^{\alpha_i}.\] <p>It’s important to highlight that in order for the exponential mixture to yield a valid probability distribution, it has to be normalized.</p> <p>The <em>exponential mixture</em> expresses \(N\) constraints \(q_i(x)\) that must be true simultaneously with precisions \(\alpha_i\). That is, \(q_1\) <strong>and</strong> \(q_2\) <strong>and</strong> … <strong>and</strong> \(q_N\) are true, with precisions \(\alpha_1\), \(\alpha_2\), …, \(\alpha_N\) respectively, expressing a <strong>conjunction</strong> of probability distributions.</p> <h2 id="building-conjunctions-and-disjunctions">Building conjunctions and disjunctions</h2> <p>So now that we’ve seen how to express conjunctions and disjunctions of distributions as exponential and linear mixtures respectively, we’ll try to find objective functions (or divergence measures, if you prefer) to build them.</p> <p><strong>Disjunctions:</strong> If I have a set of alternative hypotheses \(q_1(x)\) through \(q_N(x)\) which are true with probabilities \(w_1\) through \(w_N\) respectively, how divergent is \(p\) from this knowledge?</p> <p>The answer is</p> \[\sum_i w_i D(q_i \| p).\] <p>That is, using the KL-divergences where \(p\) is the second argument. Indeed, the minimizer is precisely the linear mixture:</p> <p>\begin{equation} \label{eq:build-linear} \arg\min_p \sum_i w_i D(q_i | p) = \sum_i w_i q_i(x). \end{equation}</p> <p><strong>Conjunctions:</strong> If I have a set of simultaneous constraints \(q_1(x)\) through \(q_N(x)\) with precisions \(\alpha_1\) through \(\alpha_N\), how far off is \(p\) from this knowlegde?</p> <p>The answer is</p> \[\sum_i \alpha_i D(p \| q_i).\] <p>That is, using the KL-divergences where \(p\) is in the first argument. And in fact, the minimizer is precisely the exponential mixture:</p> <p>\begin{equation} \label{eq:build-exponential} \arg\min_p \sum_i \alpha_i D(p | q_i) \propto \prod q_i(x)^{\alpha_i}. \end{equation}</p> <p>Equations \eqref{eq:build-linear} and \eqref{eq:build-exponential} form the core of my argument. Basically, we have found a relation between the two KL-projections and the two logical operators <strong>and</strong> and <strong>or</strong>. The two KL-divergences then measure the divergence of \(p\) from a conjunction and disjunction, respectively.</p> <h2 id="final-thoughts">Final thoughts</h2> <p>As a final comment, I’d like to point out the connection to the Bayesian framework. Let’s consider hypotheses \(h \in \mathcal{H}\) with i.i.d. likelihoods \(q(x|h)\) over \(\mathcal{X}\) and priors \(q(h)\).</p> <p><strong>Prediction:</strong> The first step is to build the predictor. Since we believe that one and only one hypothesis is true, then we take the disjunction over the likelihoods (or expert predictors), where the weights are given by the prior probabilities:</p> \[\arg\min_{p(x)} \sum_h q(h) D\big(q(x | h) \big\| p(x) \big) = \sum_h q(h) q(x|h).\] <p>The resulting estimator \(\sum_h q(h) q(x \mid h)\) is the Bayesian estimator.</p> <p><strong>Belief update:</strong> The second step is to understand how to update our beliefs after making an observation. It turns out that an observation is a constraint on the prior beliefs, i.e. we obtain the posterior through a conjunction between the prior and the likelihood function. If \(\dot{x}\) is our observation, then the conjunction is</p> \[\arg\min_{p(h)} \big\{ \alpha D\big(p(h) \big\| q(h)\big) + \beta D\big(p(h) \big\| q(\dot{x}|h)\big) \big\} \propto q(h)^\alpha q(\dot{x}|h)^\beta.\] <p>In this operation we use precisions equal to \(\alpha=\beta=1\) so that the result is proportional to \(q(h)q(\dot{x}|h)\). We can easily verify that the right hand side is indeed the Bayesian posterior after normalization.</p> <p>Notice how building the predictor involves taking the disjunction of distributions over the observations \(x\), while computing the posterior amounts to computing the conjunction of functions over the hypotheses \(h\). In doing so, we interpret the likelihood as two different functions: in the disjunction, it is regarded as a function of \(x\) whereas in the conjunction it is a function of \(h\).</p> <p>Thus, it turns out that sequential predictions can be regarded as an alternation between OR and AND operations, first to express our uncertainty over the hypotheses, and second to incorporate new evidence, respectively.</p> </d-article> <d-appendix> <d-footnote-list></d-footnote-list> <d-citation-list></d-citation-list> </d-appendix> <d-bibliography src="/assets/bibliography/"></d-bibliography> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2023 Pedro A. Ortega. </div> </footer> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-XPXFC9KLTH"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XPXFC9KLTH");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>