---
layout: distill
title: And, Or, and the two KL-projections
date: 2023-12-03 11:12:00-0400
description: Relating the two KL-projections to AND and OR
tags: and-or, KL-projections
categories: information-theory
related_posts: false

authors:
  - name: Pedro A. Ortega
---


> I discuss the difference between minimizing the KL-divergence with respect to
> the first and second argument, and will conclude that they correspond to 
> AND and OR operations on distributions, respectively.

Oftentimes I see people wonder about the meaning of the two KL-projections:

$$
    \min_p D(p \| q) \qquad \text{versus} \qquad \min_p D(q \| p),
$$

where of course (in the discrete case)

$$
    D(p \| q) := \sum_x p(x) \log \frac{ p(x) }{ q(x) }.
$$

One popular intuitive explanation is that "the first finds the mode and the second the support", motivated by depictions such as the following
(from [Manisha and Gujar, 2019](https://arxiv.org/abs/1804.00140))

<div class="row mt-1">
    <div class="col-sm mt-1 mt-md-0">
        {% include figure.html path="/assets/img/forward-reverse-kl.png" class="img-fluid rounded z-depth-1" zoomable=true alt="Pedro A. Ortega at the NIH, Bethesda (Washington DC)" %}
    </div>
</div>

Personally, I think this is a bit vague and unsatisfactory. I had battled with this
distinction since my PhD studies, trying to understand it from multiple angles (notably, 
[information geometry](https://en.wikipedia.org/wiki/Information_geometry)) but I was never
really happy with the way the material was presented in the literature. 
I think the simple explanation below condenses a significant amount of insight
without resorting to information geometry. I hope you agree!


## A tale of two coordinate systems

It's pretty hard to understand the difference between the two projections
just by applying them to two distributions $$p$$ and $$q$$. 
Rather, we'll tackle this by looking at two ways of mixing multiple distributions.

### Linear mixture

Let's say we have $$N$$ distributions $$q_1, q_2, \ldots, q_N$$ over a finite set $$X$$.
Given a set of positive weights $$w_1, w_2, \ldots, w_N$$ that sum up to one, their 
*linear mixture* is

\begin{equation}
    \label{eq:linear-mix}
    q(x) = \sum_i w_i q_i(x),
\end{equation}

The *linear mixture* expresses $$N$$ mutually exclusive hypotheses $$q_i(x)$$ that
could be true with probabilities $$w_i$$. That is, either $$q_1$$ **or** $$q_2$$ **or**
... **or** $$q_N$$ is true, with probability $$w_1$$, $$w_2$$, ..., $$w_N$$ respectively,
expressing a **disjunction** of probability distribution.

### Exponential mixture

Given a set of positive coefficients $$\alpha_1, \alpha_2, \ldots, \alpha_N$$ (but not necessarily summing up to one), their *exponential mixture* (a.k.a. geometric mixture) is

\begin{equation}
    \label{eq:exponential-mix}
    q(x) \propto \prod_i q_i(x)^{\alpha_i}.
\end{equation}

Note the exponential mixture needs to be normalized in order for the result $$q(x)$$
to be a probability distribution.

The *exponential mixture* expresses $$N$$ constraints $$q_i(x)$$ that must be true 
simultaneously with precisions $$\alpha_i$$. That is, $$q_1$$ **and** $$q_2$$ **and** ... 
**and** $$q_N$$ are true, with precisions $$\alpha_1$$, $$\alpha_2$$, ..., $$\alpha_N$$ 
respectively, expressing a **conjunction** of probability distributions.

## Building conjunctions and disjunctions

So now that we've seen how to express conjunctions and disjunctions of distributions as
exponential and linear mixtures respectively, we'll try to find objective functions (or 
divergence measures, if you prefer) to build them.

**Disjunctions:** If I have a set of alternative hypotheses $$q_1(x)$$ through $$q_N(x)$$ 
which are true with probabilities $$w_1$$ through $$w_N$$ respectively, 
how far off is $$p$$ from this knowledge?

The answer is

$$
  \sum_i w_i D(q_i \| p).
$$

That is, using the KL-divergences where $$p$$ is the second argument. Indeed, 
the minimizer is precisely the linear mixture:

\begin{equation}
  \label{eq:build-linear}
  \arg\min_p \sum_i w_i D(q_i \| p) = \sum_i w_i q_i(x).
\end{equation}

**Conjunctions:** If I have a set of simultaneous constraints $$q_1(x)$$ through $$q_N(x)$$
with precisions $$\alpha_1$$ through $$\alpha_N$$, how far off is $$p$$ from this knowlegde?

The answer is

$$
  \sum_i \alpha_i D(p \| q_i).
$$

That is, using the KL-divergences where $$p$$ is in the first argument. And in fact,
the minimizer precisely is the exponential mixture:

\begin{equation}
  \label{eq:build-exponential}
  \arg\min_p \sum_i \alpha_i D(p \| q_i) \propto \prod q_i(x)^{\alpha_i}.
\end{equation}

Equations \eqref{eq:build-linear} and \eqref{eq:build-exponential} form the core
of my argument. Basically, we have found a relation between the two KL-projections
and the two logical operators **and** and **or**. The two KL-divergences then measure
the deviation from a conjunction and disjunction, respectively.

## Final thoughts

Let's revisit the figure above:

<div class="row mt-1">
    <div class="col-sm mt-1 mt-md-0">
        {% include figure.html path="/assets/img/forward-reverse-kl.png" class="img-fluid rounded z-depth-1" zoomable=true alt="Pedro A. Ortega at the NIH, Bethesda (Washington DC)" %}
    </div>
</div>

We are equipped with a new interpretation. For simplicity, let's think of the blue
distribution ($$p_d$$) as a mixture of two Gaussians with equal weights, but different locations.
The next table gives an intuitive justification for the four cases.

| OR  | AND |
| --- | --- |
| $$p_g$$ does not capture the disjunction of the two distribution in the mixture very well, since the right peak isn't covered by it. Hence, $$D(p_d \| p_g)$$ is high.  | $$p_g$$ assigns little probability mass to any peak of the mixture, hence it is difficult for both $$p_g$$ and $$p_d$$ to be true at the same time: $$D(p_g \| p_d)$$ is high. |
| $$p_g$$ models the disjunction better and thus $$D(p_d \| p_g)$$ will be lower (albeit suboptimal). | Since $$p_g$$ covers one of the peaks of $$p_d$$, it is a bit easier for both to be true simultaneously, meaning that $$D(p_g \| p_d)$$ is lower. |

As a final comment, I'd like to point out the connection to the Bayesian framework.
Let's consider two hypotheses $$h=1,2$$ with i.i.d. likelihoods $$q(x|h=1)$$ and $$q(x|h=2)$$
and priors $$q(h=1)$$ and $$q(h=2)$$. Then, the Bayesian estimator is formed through
a disjunction (i.e. one of them must be true):

$$
  \arg\min_p \sum_i q(h=i) D\big(q(x \| h=i) | p(x) \big)
  = \sum_i q(h=i) q(x \| h=i).
$$

Observations are constraints on the estimator. If we now observe $x=1$, then we incorporate this by forming a conjunction between the prior and the likelihood function:

$$
  \arg\min_p \sum_i D\big(p(h=i) \| q(x=1 | h=i)\big)
  \propto q(h=i) q(x=1 | h=i),
$$

and the product on the right hand side is proportional to the posterior $$q(h=i | x=1)$$.

